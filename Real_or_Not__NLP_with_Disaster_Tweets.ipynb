{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real or Not? NLP with Disaster Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NQaHS-u_NF_o",
        "_TNWkv-UeDcI",
        "20dciPk0BYXs",
        "N4DjPi2AHvq3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEDyaSjj0ljp",
        "colab_type": "text"
      },
      "source": [
        "# Competition\n",
        "Twitter has become an important communication channel in times of emergency.\n",
        "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
        "\n",
        "But, it’s not always clear whether a person’s words are actually announcing a disaster.\n",
        "\n",
        "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a [quick tutorial](https://www.kaggle.com/philculliton/nlp-getting-started-tutorial) to get you up and running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6OBC9Z34fK",
        "colab_type": "text"
      },
      "source": [
        "### What files do I need?\n",
        "You'll need train.csv, test.csv and sample_submission.csv.\n",
        "\n",
        "### What should I expect the data format to be?\n",
        "Each sample in the train and test set has the following information:\n",
        "* The text of a tweet.\n",
        "* A keyword from that tweet (although this may be blank!).\n",
        "* The location the tweet was sent from (may also be blank).\n",
        "\n",
        "### What am I predicting?\n",
        "You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
        "\n",
        "### Files\n",
        "* train.csv - the training set\n",
        "* test.csv - the test set\n",
        "* sample_submission.csv - a sample submission file in the correct format\n",
        "\n",
        "### Columns\n",
        "* id - a unique identifier for each tweet\n",
        "* text - the text of the tweet\n",
        "* location - the location the tweet was sent from (may be blank)\n",
        "* keyword - a particular keyword from the tweet (may be blank)\n",
        "target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgZVHhK251ce",
        "colab_type": "code",
        "outputId": "7a36f5ae-9276-40f2-cbc4-d0d3c4f292c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30RjwR6e6StF",
        "colab_type": "code",
        "outputId": "e51c12b1-a30d-4e6f-820d-4fb8b8b73806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd ..\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks/Real\\ or\\ Not\\?\\ NLP\\ with\\ Disaster\\ Tweets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/gdrive/My Drive/Colab Notebooks/Real or Not? NLP with Disaster Tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQaHS-u_NF_o",
        "colab_type": "text"
      },
      "source": [
        "### Importing packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlsj05rC5wzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy and pandas for data manipulation\n",
        "import math\n",
        "import nltk\n",
        "from   nltk              import bigrams,trigrams\n",
        "from   nltk.collocations import *\n",
        "from   nltk.corpus       import stopwords\n",
        "from   nltk.metrics      import BigramAssocMeasures\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import string\n",
        "from string import punctuation\n",
        "import time\n",
        "\n",
        "# sklearn preprocessing\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppress warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNyr0QosOdyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "80f4d3c9-9944-4353-c43e-6d754b142bea"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TNWkv-UeDcI",
        "colab_type": "text"
      },
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJOOukCo6xog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "30f96a9c-f3ce-4f44-f3ca-bfcf33db3ac2"
      },
      "source": [
        "train_data = pd.read_csv('data/train.csv')\n",
        "train_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm1r-uSY7cZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5a35a65e-5dc9-4af8-a955-030f818f988a"
      },
      "source": [
        "test_data = pd.read_csv('data/test.csv')\n",
        "test_data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3263, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kSMb8e7BkVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to create a regular expression rule using wods\n",
        "def convert_to_rule(word1, word2):\n",
        "    return re.compile(str('(.*)('+word1+\".*\"+word2+')'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20dciPk0BYXs",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and normalize text data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMt2RVsnFh4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(text):\n",
        "  # Remove wierd characters\n",
        "  try:\n",
        "    text = str(text.encode('ascii', 'ignore'))\n",
        "  except:\n",
        "    text = 'No Information'\n",
        "\n",
        "  # Remove links/websites from text\n",
        "  text= \" \"+re.sub(r'www[.][\\S]*[.].{3}','',re.sub(r'https?:\\/\\/[\\S]*[.].{3}','',text))+\" \"\n",
        "  \n",
        "  # Calculate Emoticon score:\n",
        "  # 1. Calculate the difference between the number of positive and negative emoticons in the text\n",
        "  emoticon_score = len([emoticon for emoticon in pos_emoticons if emoticon in text]+[symbol for symbol in pos_symbols if symbol in text.lower()]) - len([emoticon for emoticon in neg_emoticons if emoticon in text]+[symbol for symbol in neg_symbols if symbol in text.lower()])\n",
        "\n",
        "  # 2. Give a score to the text from -2 to +2 depending apon the above number\n",
        "  if emoticon_score > 3: \n",
        "    emoticon_score = 2\n",
        "  elif emoticon_score > 0:\n",
        "    emoticon_score = 1\n",
        "  elif emoticon_score == 0:\n",
        "    emoticon_score = 0\n",
        "  elif emoticon_score >- 3: \n",
        "    emoticon_score =-1\n",
        "  else:\n",
        "    emoticon_score =-2\n",
        "  # Remove numbers from text\n",
        "  text = re.sub(r'[0-9]*','',text.lower())\n",
        "  text = re.sub(r'[0-9]+[0-9.,]+[0-9]+','',text)\n",
        "\n",
        "  # Replace generic phrases by corresponding normalised phrases\n",
        "  for p, phrase in enumerate(generic_phrases):\n",
        "    if phrase in text:\n",
        "      text = text.replace(phrase, normalised_phrases[p])\n",
        "  # Remove inverted commas, brackets and other punctuation marks, and multiple consecutive exclamation marks, question marks, commas, full stops, etc\n",
        "  text = re.sub(name,' @', re.sub(eclm,' ! ',re.sub(qstn,' ? ',re.sub(coma,' , ',re.sub(full,' . ',re.sub(rand,' ',text))))))\n",
        "  text = re.sub('\\/',' or ',text)\n",
        "  text = re.sub('\\[','',text)\n",
        "  text = re.sub('\\]','',text)\n",
        "  text = re.sub(nums,'',text)\n",
        "  text = re.sub(dash,' ', text)\n",
        "  text = text.replace(\"'\",'')\n",
        "  \n",
        "  # Return clean text and calculated emoticon score\n",
        "  return text, emoticon_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4DjPi2AHvq3",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis\n",
        "The first thing that we would want to add for our dataset are the sentiment score (if is not neutral) in every tweet, the sentiment words if there is and the an emoticon score in case the user used an emoticon on the tweet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Ann1WEQPgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to retrive sentiment score of a given word/bigram/phrase from sentiment dictionary\n",
        "def sentiment_score(phrase):\n",
        "  if phrase in pos_words:\n",
        "    return np.mean(list(positive_keys[positive_keys['Words']  == phrase]['Co_Word_score']))\n",
        "  elif phrase in neg_words:\n",
        "    return np.mean(list(negative_keys[negative_keys['Words']  == phrase]['Co_Word_score']))\n",
        "  elif phrase in pos_bigrm:\n",
        "    return np.mean(list(positive_keys[positive_keys['Bigrams']== phrase]['Co_Bigram_score']))\n",
        "  elif phrase in neg_bigrm: \n",
        "    return np.mean(list(negative_keys[negative_keys['Bigrams']== phrase]['Co_Bigram_score']))\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "# Function to combine scores of the same polarity\n",
        "def combine(score_list):\n",
        "    max_score = max(score_list)\n",
        "    return (max_score + ((sum(score_list) - max_score)/(math.pow(2, max_score))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDg3B5TY8t7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to roll-up word/phrase leve scores to give overall text score\n",
        "def aggregate_scores(data):\n",
        "    sent_list = list(set(list(data['sent_no'])))\n",
        "    net_scores= [0]*len(sent_list)\n",
        "    # Iterate through every sentence in the text\n",
        "    for sent_no in sent_list:\n",
        "        phrase = data[data['sent_no'] == sent_no]\n",
        "        # Extract individual scores for the sentence in a list\n",
        "        score_list = list(phrase['word_score'])                                   \n",
        "        # Extract positive scores\n",
        "        pos = [s for s in score_list if s>0]\n",
        "        # Extract negative scores\n",
        "        neg = [s for s in score_list if s<0]\n",
        "        pos_score, neg_score = 0, 0\n",
        "        # Combine positive scores to get net positive score for the sentence\n",
        "        if len(pos) > 0:\n",
        "          pos_score = combine(pos)                                 \n",
        "        # Combine negative scores to get net negative score for the sentence\n",
        "        if len(neg) > 0:\n",
        "          neg_score = (-combine([-s for s in neg]))                  \n",
        "        # Add the above two to get overall sentence score\n",
        "        net_scores[sent_no-1] = pos_score + neg_score                               \n",
        "    # Take average of all sentence scores to get overall text score\n",
        "    overall_score = round(np.mean([s for s in net_scores if s!=0]), 2)             \n",
        "    if np.isnan(overall_score): \n",
        "      return 0\n",
        "    else:\n",
        "      return overall_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaIHeeldKlJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to identify features, sentiemnt words and corresponding scores\n",
        "def idenify_features_and_describers(data):\n",
        "  # Iterate through every word in the text\n",
        "  for w in data.index:\n",
        "    word = data['word'][w]\n",
        "    if word in negation_words: \n",
        "      data['is_describer'][w] = 'REFER TO NEXT'\n",
        "\n",
        "    # Identifying sentiment words and scores\n",
        "    # 1. Word level\n",
        "    # Check if the word is in the list of positive words or negative words\n",
        "    if word in (pos_words + neg_words):  \n",
        "      # Retrive the sentiment score of the word\n",
        "      data['word_score'][w] = sentiment_score(word)             \n",
        "      # Add the word to the list of sentiment words\n",
        "      data['is_describer'][w] = word                              \n",
        "\n",
        "    # 2. Bigram level\n",
        "    if w > 0:\n",
        "      # create bigram from word and preceeding word\n",
        "      bigram = data['word'][w-1] +' '+ word\n",
        "      # Check if the bigram is in the list of positive bigrams or negative bigrams\n",
        "      if bigram in (pos_bigrm + neg_bigrm):\n",
        "        # Retrive the sentiment score of the bigram\n",
        "        data['word_score'][w] = sentiment_score(bigram)\n",
        "        data['word_score'][w-1] = 0\n",
        "        # Add the bigram to the list of sentiment words\n",
        "        data['is_describer'][w]  = bigram\n",
        "        data['is_describer'][w-1]= 'REFER TO NEXT'\n",
        "\n",
        "    # Identifying features\n",
        "    # Logic used: Word is a feature if\n",
        "    # * it is a noun\n",
        "    # * is not in the list of positive/negative words\n",
        "    # * is not a common english word like 'the','of' etc\n",
        "    # * has a more than 3 characters\n",
        "    if all([data['pos'][w] in ('NN','NNP','NNS','NNPS'), word not in pos_words + neg_words, word not in stopwords.words('english'), len(word) > 3]):\n",
        "      if w == 0:\n",
        "        data['is_entity'][w] = word\n",
        "      elif data['is_entity'][w-1] not in ['','REFER TO NEXT']:\n",
        "        data['is_entity'][w] = data['is_entity'][w-1]+' '+word\n",
        "        data['is_entity'][w-1] = 'REFER TO NEXT'\n",
        "      else:\n",
        "        data['is_entity'][w] = word\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHQTG7cLBcd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to break the text into sentences\n",
        "def identify_phrases(data):\n",
        "  # Iterate through every word in the text\n",
        "  for r in data.index:\n",
        "    if data['pos'][r] == '.':\n",
        "      # If word is a symbol like '.','?','!' then the succeeding text is treated as a new sentence\n",
        "      data['sent_no'][r+1:] = data['sent_no'][r]+1\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5qCss4VNfND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to modify word level scores based on surounding words\n",
        "def apply_surr_words_effect(data):\n",
        "  # Iterate through every word in the text\n",
        "  for w in data.index:\n",
        "    word = data['word'][w]\n",
        "    # Add the previous 3 words to the current word to create a phrase, for phrase level analysis\n",
        "    phrase = ' '.join(data['word'][max(0,w-3):w+1])\n",
        "    # Define the number of preceeding words to be considered while checking effect of negation words (currently set to 3)\n",
        "    pre_words_consider_for_negation = 3\n",
        "    preceding_start = max(0,w-pre_words_consider_for_negation)\n",
        "    # Initialize a flag to check if the text is preceeded by a negation word\n",
        "    negated=0\n",
        "    # Initialize a flag to check if the text is preceeded by an intensity word\n",
        "    intense=0\n",
        "\n",
        "    # Applying exception rules\n",
        "    # Logic:    If text contains any of the exceptions (specified in the sentiment dictionary) then its score becomes the score of that exception phrase\n",
        "    # Example:  If text contains the word 'cheap' along with the word 'price', its sentiment score becomes positive instead of negative\n",
        "    if not any([symbol for symbol in ['.','?','!'] if symbol in phrase]):\n",
        "      for rule, score in exeption_rules:\n",
        "        if rule.match(phrase) and phrase == rule.match(phrase).group(0):\n",
        "          data['word_score'][w] = score\n",
        "          data['is_describer'][w] = rule.match(phrase).group(2)\n",
        "          for p_w in range((w-len(data['is_describer'][w].split())+1), w):\n",
        "            data['is_describer'][p_w] = 'REFER TO NEXT'\n",
        "          preceding_start = max(w+1-len(data['is_describer'][w].split())-pre_words_consider_for_negation,0)\n",
        "\n",
        "    if data['word_score'][w]!=0:\n",
        "      # Applying negation rules\n",
        "      # Logic:    If there is a word like not/never/dont in the preceeding 3 words (this value is defined above), the score is negated\n",
        "      # Example: 'not good' is negative\n",
        "      preceding_text = ' '+'  '.join(data['word'][preceding_start:w+1])+' '\n",
        "      if not any([n+' ' in data['is_describer'][w] for n in negation_words]):\n",
        "        for negator in negation_words:\n",
        "          negation_rule = convert_to_rule(' '+negator+' ',' '+word+' ')\n",
        "          if negation_rule.match(preceding_text) and preceding_text==negation_rule.match(preceding_text).group(0):\n",
        "            if list(set(data['word_score'][w-len(negation_rule.match(preceding_text).group(2).split())+1:w]))==[0]: \n",
        "              negated = 1\n",
        "        if data['word_score'][w]>0 and negated==0:\n",
        "          for negator in pos_negation_words:\n",
        "            negation_rule=convert_to_rule(' '+negator+' ',' '+word+' ')\n",
        "            if negation_rule.match(preceding_text) and preceding_text==negation_rule.match(preceding_text).group(0):\n",
        "              if list(set(data['word_score'][w-len(negation_rule.match(preceding_text).group(2).split())+1:w]))==[0]: \n",
        "                negated = 1\n",
        "\n",
        "      # Applying intensity rules\n",
        "      # Logic:    If there is a word like very/soo/highly in the preceeding 3 words (this value is defined above), the score is amplified\n",
        "      # Example:  'very good' is more positive than 'good'\n",
        "      if not any([n+' ' in data['is_describer'][w] for n in intensity_words]):\n",
        "        for intensifier in intensity_words:\n",
        "          intensity_rule=convert_to_rule(' '+intensifier+' ',' '+word+' ')\n",
        "          if intensity_rule.match(preceding_text) and preceding_text==intensity_rule.match(preceding_text).group(0):\n",
        "            if list(set(data['word_score'][w-len(intensity_rule.match(preceding_text).group(2).split())+1:w])) == [0]:\n",
        "              intense=1\n",
        "      if w > 0 and data['word'][w-1] in ['pretty','quite'] and 'very' not in data['is_describer'][w]:\n",
        "        data['word_score'][w-1] = 0\n",
        "        data['is_describer'][w-1] = 'REFER TO NEXT'\n",
        "        intense = 1\n",
        "\n",
        "      if negated == 1 and intense == 1:\n",
        "        data['word_score'][w] = -data['word_score'][w] + (int(data['word_score'][w]>0) - int(data['word_score'][w]<0))\n",
        "        data['is_describer'][w] = 'not very '+ data['is_describer'][w]\n",
        "      elif negated == 1 and intense == 0:\n",
        "        data['word_score'][w] = -data['word_score'][w]\n",
        "        data['is_describer'][w] = 'not '+data['is_describer'][w]\n",
        "      elif negated==0 and intense==1:\n",
        "        data['word_score'][w] = data['word_score'][w]+(int(data['word_score'][w]>0)-int(data['word_score'][w]<0))\n",
        "        data['is_describer'][w] = 'very '+data['is_describer'][w]\n",
        "      #  Check if word is preceeded by an adverb (defined as a word ending in 'ly') and if the sentiment of the word and the adverb are opposite, ignore the sentiment of the adverb\n",
        "      if w-1 in data.index:\n",
        "        if  data['word'][w-1][-2:] == 'ly' and data['word_score'][w-1]*data['word_score'][w] < 0:\n",
        "          data['word_score'][w-1]  = 0\n",
        "        if data['word'][w-1] in ['nothing']:\n",
        "          if data['word_score'][w] < 0:\n",
        "            data['word_score'][w]= 1\n",
        "          if data['word_score'][w] > 0:\n",
        "            data['word_score'][w]=-1\n",
        "          data['is_describer'][w]='not '+data['is_describer'][w]\n",
        "\n",
        "    # Handling effect of 'yet' and'but'\n",
        "    # Logic:   If there are 2 sentiment words with the word yet/but between them, sentiment of the former is not important\n",
        "    # Example: 'tasty but ugly' is negative, 'ugly yet tasty' is positive\n",
        "    if word in ['yet','but']:\n",
        "      #  Check if any of the 3 words after the word but/yet have a non-zero sentiment, and if present, extract its score\n",
        "      suc_score=0\n",
        "      if w + 1 in data.index and data['word_score'][w+1] != 0:\n",
        "        suc_score=data['word_score'][w+1]\n",
        "      elif w + 2 in data.index and data['word_score'][w+2] !=0:\n",
        "        suc_score=data['word_score'][w+2]\n",
        "      elif w + 3 in data.index and data['word_score'][w+3] != 0:\n",
        "        suc_score=data['word_score'][w+3]\n",
        "      # Check if any of the 3 words before the worc but/yet has a sentiment opposite to that of the above found word, and if present, make its score zero\n",
        "      if suc_score!=0:\n",
        "        if   w-1 in data.index and data['word_score'][w-1]*suc_score<0:\n",
        "          data['word_score'][w-1]=0\n",
        "        elif w-2 in data.index and data['word_score'][w-2]*suc_score<0:\n",
        "          data['word_score'][w-2]=0\n",
        "        elif w-3 in data.index and data['word_score'][w-3]*suc_score<0:\n",
        "          data['word_score'][w-3]=0\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_8HzneaNjcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the sentiment score of the text\n",
        "def text_sentiments(survey):\n",
        "  # Remove unnessecary punctuations, numbers, website links etc to get clean simple text\n",
        "  text, emoticon_score = clean(survey)\n",
        "  # Extract words fromt the text\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  # Create a table to store metadata about all the words like their part of speech, sentiment score etc\n",
        "  data = pd.DataFrame({'word':tokens,'pos':[tag for (word,tag) in nltk.pos_tag(tokens)]}) \n",
        "  data['word_score'], data['is_entity'], data['is_describer'], data['sent_no'] = 0, '', '', 1\n",
        "  # Score the words/phrases\n",
        "  data = apply_surr_words_effect(identify_phrases(idenify_features_and_describers(data))) \n",
        "  # Combine above scores to get overall text score\n",
        "  overall_score = aggregate_scores(data)\n",
        "  # Extract the words/phrases with non-zero sentiment scores\n",
        "  senti_word_data=data[data['word_score']!=0]\n",
        "  # Combine the above words/phrases into a single string\n",
        "  sentiment_words=','.join([w for w in list(set(list(senti_word_data['is_describer']))) if w not in ['REFER TO NEXT','']])\n",
        "  return overall_score, sentiment_words, emoticon_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCR9_hqhMych",
        "colab_type": "text"
      },
      "source": [
        "#### Load sentiment dictionary\n",
        "\n",
        "Load sentiment dictionary from General_Sentiment_Lexicon_v5.xlsx and create a list of sentiment related words, bigrams, emoticons, symbols and other phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sshVZwkoNW8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define constants:\n",
        "\n",
        "# Load the sentiement dictionary from excel file\n",
        "sentiment_dictionary = 'General_Sentiment_Lexicon_v5.xlsx'\n",
        "positive_keys        = pd.read_excel(sentiment_dictionary,'Positive Phrases')\n",
        "negative_keys        = pd.read_excel(sentiment_dictionary,'Negative Phrases')\n",
        "amplifier_keys       = pd.read_excel(sentiment_dictionary,'Negators and Amplifiers')\n",
        "standard_keys        = pd.read_excel(sentiment_dictionary,'Standardization Rules')\n",
        "exeption_keys        = pd.read_excel(sentiment_dictionary,'Exeption Rules')\n",
        "\n",
        "# Create lists for sentiment related words,bigrams,emoticons,symbols and other phrases\n",
        "pos_emoticons        = [str(e) for e in positive_keys['Emoticons']   if not str(e)=='nan']\n",
        "neg_emoticons        = [str(e) for e in negative_keys['Emoticons']   if not str(e)=='nan']\n",
        "pos_symbols          = [str(s) for s in positive_keys['Symbols']     if not str(s)=='nan']\n",
        "neg_symbols          = [str(s) for s in negative_keys['Symbols']     if not str(s)=='nan']\n",
        "pos_words            = [str(w) for w in positive_keys['Words']       if not str(w)=='nan']\n",
        "neg_words            = [str(w) for w in negative_keys['Words']       if not str(w)=='nan']\n",
        "pos_bigrm            = [str(w) for w in positive_keys['Bigrams']     if not str(w)=='nan']\n",
        "neg_bigrm            = [str(w) for w in negative_keys['Bigrams']     if not str(w)=='nan']\n",
        "intensity_words      = [str(w) for w in amplifier_keys['Amplifiers'] if not str(w)=='nan']\n",
        "negation_words       = [str(w) for w in amplifier_keys['Negators']   if not str(w)=='nan']\n",
        "pos_negation_words   = [str(w) for w in amplifier_keys['Positive only negators']   if not str(w)=='nan']\n",
        "normalised_phrases   = [str(w).replace('_',' ').lower() for w in standard_keys['Normalised_phrase']]\n",
        "generic_phrases      = [str(w).replace('_',' ').lower() for w in standard_keys['Phrase']]\n",
        "StopWords            = list(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm5MJdOfG59l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map the exception rules to their corresponding sentiment sores\n",
        "exeption_rules=[]\n",
        "for k in exeption_keys.index:\n",
        "  if str(exeption_keys['Preceeding'][k])!='nan':\n",
        "    for pre in exeption_keys['Preceeding'][k].split(','):\n",
        "      exeption_rules.append((convert_to_rule(pre,exeption_keys['Word'][k]),exeption_keys['Sentiment'][k]))\n",
        "  if str(exeption_keys['Suceeding'][k])!='nan':\n",
        "    for suc in exeption_keys['Suceeding'][k].split(',') :\n",
        "      exeption_rules.append((convert_to_rule(exeption_keys['Word'][k],suc),exeption_keys['Sentiment'][k]))\n",
        "  if str(exeption_keys['Around'][k])!='nan':\n",
        "    for sur in exeption_keys['Around'][k].split(','):\n",
        "      exeption_rules.append((convert_to_rule(sur,exeption_keys['Word'][k]),exeption_keys['Sentiment'][k]))\n",
        "      exeption_rules.append((convert_to_rule(exeption_keys['Word'][k],sur),exeption_keys['Sentiment'][k]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpBH25hVSkFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define regur expressions for punctuations and symbols\n",
        "rand    = re.compile('[~`|#$)%^}&{(*<>\"_+:;]')\n",
        "full    = re.compile('[.]+')\n",
        "coma    = re.compile('[,]+')\n",
        "qstn    = re.compile('[?]+')\n",
        "eclm    = re.compile('[!]+')\n",
        "name    = re.compile('[@]+')\n",
        "dash    = re.compile('[-]+')\n",
        "nums    = re.compile('\\S*[\\d]\\S*')\n",
        "periods = re.compile('[!]+|[.]+|[?]+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUoGy9ztIUBS",
        "colab_type": "text"
      },
      "source": [
        "#### Add Sentiment Analysis Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXzUV9vQJg3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets create a copy of both datasets just to have a variable were the data is not being manipulated\n",
        "train_data_copy = train_data.copy()\n",
        "test_data_copy = test_data.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61kaTCPKDiEy",
        "colab_type": "text"
      },
      "source": [
        "We add this three features to the training set and the testing set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbP_QVKyQJCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2e8d06aa-a86d-461a-e5c6-dfec7497b722"
      },
      "source": [
        "train_data_copy['Text sentiment'] = 0.0\n",
        "train_data_copy['Sentiment words'] = ''\n",
        "train_data_copy['Emoticon sentiment'] = 0\n",
        "\n",
        "for i in train_data_copy.index:\n",
        "  train_data_copy['Text sentiment'][i], train_data_copy['Sentiment words'][i], train_data_copy['Emoticon sentiment'][i] = text_sentiments(train_data_copy['text'][i])\n",
        "train_data_copy.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>Text sentiment</th>\n",
              "      <th>Sentiment words</th>\n",
              "      <th>Emoticon sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>smoke</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location  ... Text sentiment  Sentiment words  Emoticon sentiment\n",
              "0   1     NaN      NaN  ...            0.0                                    0\n",
              "1   4     NaN      NaN  ...            0.0                                    0\n",
              "2   5     NaN      NaN  ...            0.0                                    0\n",
              "3   6     NaN      NaN  ...            0.0                                    0\n",
              "4   7     NaN      NaN  ...           -2.0            smoke                   0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPC9_FTSw-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d5267f4b-c56d-4bee-87f1-6cd4b3b0fbc5"
      },
      "source": [
        "test_data_copy['Text sentiment'] = 0.0\n",
        "test_data_copy['Sentiment words'] = ''\n",
        "test_data_copy['Emoticon sentiment'] = 0\n",
        "\n",
        "for i in test_data_copy.index:\n",
        "  test_data_copy['Text sentiment'][i], test_data_copy['Sentiment words'][i], test_data_copy['Emoticon sentiment'][i] = text_sentiments(test_data_copy['text'][i])\n",
        "test_data_copy.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>Text sentiment</th>\n",
              "      <th>Sentiment words</th>\n",
              "      <th>Emoticon sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>terrible,crash</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>2.00</td>\n",
              "      <td>safe</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>fleeing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>0.00</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>-2.00</td>\n",
              "      <td>kills</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location  ... Text sentiment  Sentiment words Emoticon sentiment\n",
              "0   0     NaN      NaN  ...          -3.25   terrible,crash                  0\n",
              "1   2     NaN      NaN  ...           2.00             safe                  0\n",
              "2   3     NaN      NaN  ...          -1.00          fleeing                  0\n",
              "3   9     NaN      NaN  ...           0.00                                   0\n",
              "4  11     NaN      NaN  ...          -2.00            kills                  0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCjOxJv2OP9y",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nOnnqazOSyl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e94c3112-e7e5-4222-ef78-526472729924"
      },
      "source": [
        "# target balance.\n",
        "train_data_copy['target'].astype(int).plot.hist()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2f88e67780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c51Wid66Cj2i",
        "colab_type": "text"
      },
      "source": [
        "#### Encoding Categorical Variables\n",
        "Before we go any further, we need to deal with pesky categorical variables like the ones in the 'Sentiment Words' column. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n",
        "\n",
        "**Label encoding:** assign each unique category in a categorical variable with an integer. No new columns are created.\n",
        "\n",
        "Problem: The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want.\n",
        "\n",
        "**One-hot encoding:** create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n",
        "\n",
        "Problem: The number of features can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions.\n",
        "\n",
        "For this case in particular, we would use One-Hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6x0NO3dCrSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "56887daa-01a2-48d5-a6b8-f671286d0a57"
      },
      "source": [
        "print('Number of each type of column')\n",
        "print(train_data_copy.dtypes.value_counts())\n",
        "print('\\nNumber of unique classes in each object column')\n",
        "print(train_data_copy.select_dtypes('object').apply(pd.Series.nunique, axis = 0))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of each type of column\n",
            "object     4\n",
            "int64      3\n",
            "float64    1\n",
            "dtype: int64\n",
            "\n",
            "Number of unique classes in each object column\n",
            "keyword             221\n",
            "location           3341\n",
            "text               7503\n",
            "Sentiment words    2985\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vqitZKZEiqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Function that separates the sentiment words into columns\n",
        "def separate_list_2_col(df, column_name, string_column=True, separator=',', remove_column=False):\n",
        "  if string_column:\n",
        "    df[column_name] = df[column_name].apply(lambda x : x.split(separator))\n",
        "  df[column_name].apply(lambda x : x.sort())\n",
        "  max_length = df[column_name].map(lambda x: len(x)).max()\n",
        "  \n",
        "  column_names = []\n",
        "  for i in range(max_length):\n",
        "    new_column_name = column_name + '_' + str(i)\n",
        "    column_names.append(new_column_name)\n",
        "    df[new_column_name] = ''\n",
        "  \n",
        "  for i in df[column_name].index:\n",
        "    for index, text in enumerate(df.iloc[i][column_name]):\n",
        "      df.at[i, column_names[index]] = text\n",
        "  \n",
        "  if remove_column:\n",
        "    df = df.drop(labels=column_name, axis=1)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLi05OD4iLtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_copy = separate_list_2_col(train_data_copy, 'Sentiment words', remove_column=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3sM1R63t-I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_copy = separate_list_2_col(test_data_copy, 'Sentiment words', remove_column=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAQIcf8w08s",
        "colab_type": "text"
      },
      "source": [
        "#### Normalizing Location\n",
        "We know that people can change the location in tweet, they can leave it in blank and even put a string that it's not a location at all.\n",
        "\n",
        "Let's try to normalize this data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pee9_jQszQ0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "def what_location(this_location):\n",
        "  this_location, _ = clean(this_location)\n",
        "  locations_replace = []\n",
        "  try:\n",
        "    for X in nlp(this_location).ents:\n",
        "      if X.label_ == 'GPE':\n",
        "        locations_replace.append(X.text)\n",
        "  except:\n",
        "    pass\n",
        "  return locations_replace\n",
        "\n",
        "def is_it_a_real_location(df, column_name, remove_column=False):\n",
        "  # progress_apply\n",
        "  df[column_name] = df[column_name].apply(lambda x : what_location(x))\n",
        "  df = separate_list_2_col(df, column_name, string_column=False, remove_column=remove_column)\n",
        "  return df\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jcYU9IkuYPB",
        "colab_type": "text"
      },
      "source": [
        "Apply this change into the train and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4TgnLq0__3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "79daacf0-91f1-4eaf-8b10-4e61a930498c"
      },
      "source": [
        "train_data_copy = is_it_a_real_location(train_data_copy, column_name='location', remove_column=True)\n",
        "train_data_copy.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>Text sentiment</th>\n",
              "      <th>Emoticon sentiment</th>\n",
              "      <th>Sentiment words_0</th>\n",
              "      <th>Sentiment words_1</th>\n",
              "      <th>Sentiment words_2</th>\n",
              "      <th>Sentiment words_3</th>\n",
              "      <th>Sentiment words_4</th>\n",
              "      <th>Sentiment words_5</th>\n",
              "      <th>Sentiment words_6</th>\n",
              "      <th>Sentiment words_7</th>\n",
              "      <th>Sentiment words_8</th>\n",
              "      <th>location_0</th>\n",
              "      <th>location_1</th>\n",
              "      <th>location_2</th>\n",
              "      <th>location_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>smoke</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... location_2  location_3\n",
              "0   1     NaN  ...                       \n",
              "1   4     NaN  ...                       \n",
              "2   5     NaN  ...                       \n",
              "3   6     NaN  ...                       \n",
              "4   7     NaN  ...                       \n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTqA0Eb-qpX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "85333def-9f49-4400-d45c-2d833fb7478b"
      },
      "source": [
        "test_data_copy = is_it_a_real_location(test_data_copy, column_name='location', remove_column=True)\n",
        "test_data_copy.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>text</th>\n",
              "      <th>Text sentiment</th>\n",
              "      <th>Emoticon sentiment</th>\n",
              "      <th>Sentiment words_0</th>\n",
              "      <th>Sentiment words_1</th>\n",
              "      <th>Sentiment words_2</th>\n",
              "      <th>Sentiment words_3</th>\n",
              "      <th>Sentiment words_4</th>\n",
              "      <th>Sentiment words_5</th>\n",
              "      <th>Sentiment words_6</th>\n",
              "      <th>location_0</th>\n",
              "      <th>location_1</th>\n",
              "      <th>location_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>0</td>\n",
              "      <td>crash</td>\n",
              "      <td>terrible</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0</td>\n",
              "      <td>safe</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>0</td>\n",
              "      <td>fleeing</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "      <td>-2.00</td>\n",
              "      <td>0</td>\n",
              "      <td>kills</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ... location_1  location_2\n",
              "0   0     NaN  ...                       \n",
              "1   2     NaN  ...                       \n",
              "2   3     NaN  ...                       \n",
              "3   9     NaN  ...                       \n",
              "4  11     NaN  ...                       \n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmIpQaed3UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Function to calculate missing values by column\n",
        "def what_missing_values(df):\n",
        "  missing_values = df.isnull().sum()\n",
        "  missing_values_percentage = (100 *  missing_values)/len(df)\n",
        "  missing_values_table = pd.concat([missing_values, missing_values_percentage], axis=1)\n",
        "  missing_values_table_renamed = missing_values_table.rename(columns={0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "  missing_values_table_renamed = missing_values_table_renamed[missing_values_table_renamed.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
        "  print(\"Number of columns: \" + str(df.shape[1]))\n",
        "  print(\"There are \" + str(missing_values_table_renamed.shape[0]) +\" columns that have missing values.\")\n",
        "  return missing_values_table_renamed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzJKgDgIeCGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "ed858a70-3635-42d6-aba5-657c3d7c4cbd"
      },
      "source": [
        "miss_value = what_missing_values(train_data)\n",
        "miss_value.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of columns: 5\n",
            "There are 2 columns that have missing values.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Values</th>\n",
              "      <th>% of Total Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>location</th>\n",
              "      <td>2533</td>\n",
              "      <td>33.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>keyword</th>\n",
              "      <td>61</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Missing Values  % of Total Values\n",
              "location            2533               33.3\n",
              "keyword               61                0.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbeslYFdZTh",
        "colab_type": "text"
      },
      "source": [
        "### Text Vectorization\n",
        "Text vectorization, or feature extraction, is the process of converting text into a numerical representation that can then be used as an input for a machine learning model or some other mathematical algorithm.  This is typically the first and most important step of any NLP task.  This notebook includes step-by-step instructions for performing a variety of text vectorization techniques on text documents.  These techniques include:\n",
        "\n",
        "* Tf-idf\n",
        "* Bag-of-words\n",
        "* Hashing\n",
        "* Sequence IDs\n",
        "* One-hot encodings\n",
        "* Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgpvD1YPaYUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a set of parameters for our count vectorizers\n",
        "params = {\n",
        "    'lowercase' : True, # lowercase all letters\n",
        "    'stop_words' : stopwords.words('english') + list(punctuation), # stop words to remove\n",
        "    'n_features' : 800, # size of each vector, indicating how many words to consider.  a good value is 1,000. We\n",
        "    'binary' : False, # track occurance rather than frequency of words if True\n",
        "    'ngram_range' : (1, 1) # range of n-grams to include, i.e. (1, 3) includes 1-, 2-, and 3-grams\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo5roz59YS51",
        "colab_type": "text"
      },
      "source": [
        "#### Tf - idf\n",
        "A term frequency - inverse document frequency (tf-idf) vectorizer describes the occurence of rare words in a document by measuring the frequency of each word in a document against how infrequently each word appears in other documents.  In other words, a word that occurs frequently in a document but infrequently in other documents will receive a high score.  A tf-idf vectorizer defines a vocabulary of known words and will construct a feature vector for each document based on the tf-idf weighting of each vocabulary word.\n",
        "\n",
        "In mathematical terms, the weight of each word in a document is given by:\n",
        "\n",
        "\\begin{equation*}\n",
        "W = T * log {\\frac{N}{D}}\n",
        "\\end{equation*}\n",
        "\n",
        "Where $W$ is the word score, $T$ is the number of occurences of the word in the document, $N$ is the number of documents in the corpus, and $D$ is the number of documents containing the word.\n",
        "\n",
        "e.g.\n",
        "\n",
        "    corpus: \n",
        "        the brown fox jumped over the brown dog\n",
        "        the quick fox jumped\n",
        "        fox jumped high\n",
        "        the dog is brown\n",
        "\n",
        "    document: the brown fox jumped over the brown dog\n",
        "\n",
        "    vocabulary: [a, the, of, brown, fox, house, jumped, over, with, dog]\n",
        "\n",
        "    feature vector: [0, 2*log(4/3), 0, 2*log(4/2), 1*log(4/3), 0, 1*log(4/3), 1*log(4/1), 0, 1*log(4/2)]\n",
        "                    [0, 0.575, 0, 1.386, 0.288, 0, 0.288, 1.386, 0, 0.693]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGDyiTZ68UVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy4BM-sUbPOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a Tfidf vectorizer object using our parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    lowercase=params['lowercase'],\n",
        "    stop_words=params['stop_words'],\n",
        "    max_features=params['n_features'],\n",
        "    binary=params['binary'],\n",
        "    ngram_range=params['ngram_range']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJGfl9mpdxSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " train_data_tfidf = train_data_copy.copy()\n",
        " test_data_tfidf = test_data_copy.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcYTsdEDbpV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit vectorizer using our train texts and transform both our train and test sets into feature vectors.\n",
        "# Feature vectors are returned as sparse vectors\n",
        "train_tfidf_features = tfidf_vectorizer.fit_transform(train_data_tfidf['text'])\n",
        "test_tfidf_features = tfidf_vectorizer.transform(test_data_tfidf['text'])\n",
        "\n",
        "# Convert to dense vectors\n",
        "train_tfidf_features = train_tfidf_features.todense()\n",
        "test_tfidf_features = test_tfidf_features.todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnUcPIo0fLLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_tfidf.drop(labels=['text'], axis=\"columns\", inplace=True)\n",
        "train_tfidf_features = pd.DataFrame(train_tfidf_features)\n",
        "train_data_tfidf = pd.concat([train_data_tfidf, train_tfidf_features], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ijzkxQrM_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_tfidf.drop(labels=['text'], axis=\"columns\", inplace=True)\n",
        "test_tfidf_features = pd.DataFrame(test_tfidf_features)\n",
        "test_data_tfidf = pd.concat([test_data_tfidf, test_tfidf_features], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud0ZAg-S3ZBI",
        "colab_type": "text"
      },
      "source": [
        "For the labels on the several locations and the sentyments in each tweet we would use One-Hot Encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45np6ZS06rFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87658fa4-5da0-4d97-be0e-ce334714296d"
      },
      "source": [
        "train_data_tfidf = pd.get_dummies(train_data_tfidf)\n",
        "test_data_tfidf = pd.get_dummies(test_data_tfidf)\n",
        "\n",
        "print('Train shape:' +  str(train_data_tfidf.shape))\n",
        "print('Test shape:' + str(test_data_tfidf.shape))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:(7613, 4064)\n",
            "Test shape:(3263, 2885)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3RkY12b-AwQ",
        "colab_type": "text"
      },
      "source": [
        "There need to be the same features (columns) in both the training and testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMf7TdTF7DNO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa3c9fb1-f2d9-404a-c183-f4a951c5c21f"
      },
      "source": [
        "train_labels = train_data_tfidf['target']\n",
        "\n",
        "train_data_tfidf, test_data_tfidf = train_data_tfidf.align(test_data_tfidf, join='inner', axis=1)\n",
        "\n",
        "train_data_tfidf['target'] = train_labels\n",
        "\n",
        "print('Training Features shape: ', train_data_tfidf.shape)\n",
        "print('Testing Features shape: ', test_data_tfidf.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Features shape:  (7613, 2228)\n",
            "Testing Features shape:  (3263, 2227)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-Ky16WPL7v",
        "colab_type": "text"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KwiRU8ZVYap",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression\n",
        "Logistic regression is named for the function used at the core of the method, the logistic function. The logistic function, also called the sigmoid function, it’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n",
        "\n",
        "\\begin{equation*}\n",
        "y =  {\\frac{1}{1-e^{-x}}}\n",
        "\\end{equation*}\n",
        "\n",
        "In Logistic Regression, input values $x$ are combined linearly using weights or coefficient values to predict an output value $y$. A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value.\n",
        "\n",
        "\\begin{equation*}\n",
        "y = {\\frac{e^{b_0 + b_1 x}}{1 + e^{b_0 + b_1 x}}}\n",
        "\\end{equation*}\n",
        "\n",
        "Where $y$ is the predicted output, $b_0$ is the bias or intercept term and $b_1$ is the coefficient for the single input value $x$. Each column in your input data has an associated $b$ coefficient (a constant real value) that must be learned from your training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPXHNUtPLvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression = LogisticRegression(C=1e5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im9kFkrqaPNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data\n",
        "train_labels = train_data_tfidf['target']\n",
        "train_data_tfidf.drop('target', axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data_tfidf, train_labels, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwPOH0G7AUGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b88971ec-7973-49d7-f0ac-192763df9c47"
      },
      "source": [
        "logistic_regression.fit(X_train, y_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ExJTeMBZ2p",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "It is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values. Read more about the confusion matrix in [this](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) link.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32kwweGVIDZU",
        "colab_type": "text"
      },
      "source": [
        "#### CM - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1tIwONOHmCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1b5c7ce-83d2-4e4b-b044-42719d4fafec"
      },
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logistic_regression.score(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMRhdwKlCFZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make prediction in the test data\n",
        "predictions = logistic_regression.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbJXPBPiHyQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdpWMoHFH2Lx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "1342190d-8a8e-4555-b959-8b72d2d0796a"
      },
      "source": [
        "cm = metrics.confusion_matrix(y_test, predictions)\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAH9CAYAAAB7vlRpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7xd853/8dcnF7cgiUiCoCjjUj+3\nqlJth7grgkFpR1W1aTu0Q686RpVqq5cZ2hqdZkpFq1r3UOoWFCUqFeJeoUKChIgg4pKcz++PtRJb\nnH3OyZKds8/Zr2cf63H2/q7bd5+Z2J/z/n7XWpGZSJIktadPd3dAkiQ1LwsFSZJUl4WCJEmqy0JB\nkiTVZaEgSZLqslCQJEl19evuDkiS1FO9Np+G32NghX5Eo8/RERMFSZJUl4mCJEkVtcI9C00UJElS\nXSYKkiRVlI2fogDdO0XBREGSJNVnoiBJUlXOUZAkSa3MREGSpIpaIFAwUZAkSfWZKEiSVJH3UZAk\nSS3NREGSpIpa4T4KFgqSJFXl0IMkSWplJgqSJFXUAoGCiYIkSarPREGSpIq8PFKSJLU0EwVJkipa\nNpdHdi8TBUmSVJeJgiRJFTlHQZIktTQLBUmSVJeFgiRJqss5CpIkVeQcBakJRMQ/IiIjYsPu7kuz\niYjNI+LyiHgmIuaVv6vfR8Tm3d23JRURy0XEjyPi1vKzdPk/wVH4j4h4qtz3lojYqpH9lVqFhYKa\nWkTsAKxXvj2sG7vSdMrCaQKwKnAM8DHgNGB1YItu7FpVKwGfBV4Fbl/CfY8HTgR+COwLvALcEBFr\nLNUeSovJZfC/7hbZCrmJeqyI+BnwGeB+YNXM3KybuwRARPQF+mbmG93Yh+8BXwDWyszXF1sX2eB/\n3BGxYmbOW8rHjMzMiDgG+HlmRhf2WQGYAfxXZp5Stg0AngB+mZn/uTT7KNWaNXd+w79Ehwzo1+m/\ng0YyUVDTKr+MDwGuAM4BNo2ILdvZ7qMRcVNEvBIRcyLi5ojYumb9eyLigoh4PiJejYjJEfGJct1O\n5bDG5osd8+aIuLjm/bkRMTEi9o+IB4DXgA9GxJoRcU5EPF5G3n+PiFMjYrnFjrdiRPwoIqZGxOvl\nEMEPynU/KvePxfb5dES8ERFD6/yKBgEvLl4kACxeJETEARHx17KPsyLi6oh4T836kRFxZ0S8FhEz\nIuKsiFi5Zv3C39MeEXFFRLwCnFmuW7cc7nih/P1eGxEb1+lzhyoWNx+iSFUurDnOXOBKYK8q/ZC6\nKrPxS3ezUFAz2xkYDvweuBh4k8WGHyJiJ2B8ue4I4OPArcCIcv0w4A7gA8DXKGLps4F1KvRnPeBH\nwA8ovoD+QRHzvwB8BdgT+DFwJPDzmj4GMA74IvA/wN7ASeW+UBRB6wP/vNj5jgSuzMzn6vTnbmCD\niPhpRNRNWiLicOBS4DGKwutI4O/A0HL9+4BrgOeBfyn79gmK3/nizgbuBfYDzo6I1YDbgI0p0o1D\ngAEUsf+KNX24OSJurtfHd2kTYAHw6GLtD5XrJL0LXvWgZnYY8CJwTWa+ERHXAYdGxLdq/vL8AcUX\n1x41bdfUHOM4YCDw/sx8pmwbX7E/Q4BdM/OemrZpFAUIABHxF2AucE5EfKkcmtgd2A0YlZlX1Ox7\nHkBmPlzudyRwc3mcDYCPUHwh1zO2PPaXgS9HxAvA1cBPM3NieZw+FPMWLsvM2iKrth8nAlOB/TJz\nQbnfC8AfImKHzLyjZtuLMvPEms/7XYrCYKvMfKHmd/AExZDR/5SbLujgc7xbg4FXFva9xmxgpYhY\nrjuHiNS7NcEf/A1noqCmVEb3B1J8wS38j/zvgfcAO5TbDAA+CIztILIeSVFoPFNn/ZKYvliRsHC2\n/bER8WBEzKNINs4HlgfWrenDC4sVCYs7G/iXmrj/0xTj7tfU2yEz52fmx4EtKb7s/0bxF/0dEfGx\ncrONgbWAX3dw7u0ofs+1X7SXAPOBDy+27VWLvd8VuB54KSL6RUQ/4OWyL9vW9HWXzNylgz5IalIW\nCmpWe1GMwV8dEYMiYhDFX9uv89bww2AggI6KgCGdrF8SM9ppOxb4CXAZMIriS/foct0KS9CHC4E2\n4JByqOII4LzMnN9ZpzJzcmaempm7UxQGzwCn1pybTs6/Jot9trJomAWstti2i/8OVqcY7nlzsWVn\nqg3vVDEbWLmc01JrMPCqaYIaKpfB0s0celCzWlgMXNTOuoMj4liKL4g2ii+6emZ1sv618udyi7UP\nphizr9XeP9mDgYsz84SFDe3MF+isD2Tm3Ij4PUWSMJUijegoBah3nCci4iLg32rOTSfnfwYYVttQ\nfukOoZh/8bZTLPb+BYphjO+2c9yXu9LnpeBhoC+wIfBITfsm5TqpYZrh8sVGM1FQ0ymHFPYFLqD4\ny7R2+QrFBMeR5cz2O4FPLX7FQI3xwB4RMbzO+mnlz01rzr8OXZ8EtyJFylHrk+30YbWI2KeTY51N\nMS/hO8CEzOzwS66cqNmejXjrL/9HgOkUCUU9dwIHLPYX+YEUf0jc1kmfxwPvAx7IzImLLY90su/S\ncjvwEkXRBkBErETx/0N/WkZ9kHotEwU1o1EUN9/5aWbeWbuinCh3AkXicD3FjXZuAP4UEWMoJhLu\nAEzMzD8CpwOfAm6N4r4DT1EUBQMy80eZOS0iJgLfjYhXKYrn/+Cdf0nXcz3FRMI7Ka4q+CTFX7aL\nb3Mt8LuIOIXiaoU1gY9m5ucXbpSZd5aXXn4Y+DydOzGKy0V/RzHDfwDFF/y+lBMsM7MtIr4BnB8R\n51MUX0kxb+KCctLjqcAk4PKI+AWwNsWNi65dbCJje/4b+Ffgxoj4OUVRMpziCo7bMvMCgIgYX/an\nw3kKEbFX+Tm2Kt8fVK66KzOnlm1nA/+cmRuWx3wtIk4rfx+zKVKEr1D83/LnSA3UDJcvNpqFgprR\nYcCjixcJAJn5ZkRcCHwiIr6YmbdExG4U0fdvgTcov/TK7Z+LiB0pLms8g2KS4aMUV0vUnu9X5f7T\ngG9QXC3RFadQXGa4cE7ApRRXIVxZ0+eMiAPKPh5bbv80xRf84i4HNqCYuNmZ84GVga9SXA76KsVl\nj4dl5qL9M/N3EfEaRYF1MUUxNQF4rlz/QPkF/f2y/y9RFBTf6KwDmfl8RGwPfI+iKBtEMZRxGzC5\nZtPF5w/U8wuKCasLLRx6OhI4t+ZYi/+36zSKwuBbFEMmE4HdMrO9eSWSloB3ZpSaSET8FXgkMw/v\n7r5I6tyzL73Z8C/RNVbt3613ZjRRkJpARGxLMRzwAd66akKSup2FgtQc7qK4udS3MvOu7u6MpC5q\ngVDeQkFqAl15+JEkdQcLBUmSKmqF+yg0c6HQ+3/7kqRGMqlbCpq5UOC1Tm9eK7WmFcp/uStufUz3\ndkRqUvMmnblMztMMFw5GxHHAZyn+wL6P4nLiNSkusx5C8eyVw8uH6y1P8UC691PcufXjmflER8f3\nzoySJPVQETGC4t4t22bm5hT3GTmU4qZpp5c3JpsNHFXuchQwu2w/vdyuQxYKkiRV1CTPhOoHrFg+\nvXUlipuejaS4wRoUj6Tfv3w9qnxPuX6XDm6BD1goSJLU1CJidERMrFlGL1yXmdMpnmD7JEWBMIdi\nqOHFmqfPTqO4eyvlz6fKfeeX2y98ymy7mnqOgiRJzWxZzFHIzDHAmPbWRcRgipRgfYp7sVwE7Lk0\nz2+iIElSz7Ur8I/MfC4z36R4XsuOwKByKAKKB71NL19PB9YBKNcP5K3H0bfLQkGSpMq6fZbCk8D2\nEbFSOddgF+BB4CZg4dNXjwDGla+v4K3Hzh8E3JidPPTJQkGSpB6qfMruxRSPr7+P4nt9DPBN4CsR\nMYViDsLZ5S5nA0PK9q8Ax3d2DucoSJJUUTPcRyEzTwJOWqz5cWC7drZ9DTh4SY5voiBJkuoyUZAk\nqaImCBQazkJBkqSKmmHoodEcepAkSXWZKEiSVFErPGbaREGSJNVloiBJUlW9P1AwUZAkSfWZKEiS\nVFELBAomCpIkqT4TBUmSKvI+CpIkqaWZKEiSVJH3UZAkSS3NREGSpKp6f6BgoiBJkuozUZAkqaIW\nCBRMFCRJUn0mCpIkVeR9FCRJUkszUZAkqSLvoyBJklqaiYIkSVX1/kDBREGSJNVnoiBJUkUtEChY\nKEiSVJWXR0qSpJZmoiBJUkVeHilJklqaiYIkSVX1/kDBREGSJNVnoiBJUkUtECiYKEiSpPpMFCRJ\nqsj7KEiSpJZmoiBJUkXeR0GSJLU0EwVJkqrq/YGCiYIkSarPREGSpIpaIFAwUZAkSfWZKEiSVJH3\nUZAkSS3NREGSpIq8j4IkSWppJgqSJFXV+wMFEwVJkqrKZbB0JCI2joh7apaXIuLYiFgtIq6PiEfL\nn4PL7SMifhYRUyJickRs09lntFCQJKmHysxHMnOrzNwKeD/wKnAZcDwwPjM3AsaX7wH2AjYql9HA\nLzo7h4WCJEkVZTZ+WQK7AI9l5lRgFDC2bB8L7F++HgWcl4UJwKCIWLOjg1ooSJLUxCJidERMrFlG\n19n0UOCC8vXwzHymfP0sMLx8PQJ4qmafaWVbXU5mlCSpomVxeWRmjgHGdLRNRCwH7Ad8q539MyIq\nd9REQZKknm8v4O7MnFG+n7FwSKH8ObNsnw6sU7Pf2mVbXRYKkiRV1d2XPbzlMN4adgC4AjiifH0E\nMK6m/VPl1Q/bA3Nqhija5dCDJEk9WEQMAHYDPl/TfBpwYUQcBUwFDinbrwb2BqZQXCFxZGfHt1CQ\nJKmiZrjfUmbOBYYs1jaL4iqIxbdN4OglOb5DD5IkqS4TBUmSKvIx05IkqaWZKEiSVJGPmZYkSS3N\nREGSpKp6f6BgoiBJkuozUZAkqaIWCBRMFCRJUn0mCpIkVeR9FCRJUkszUZAkqaJWuI+ChYIkSVX1\n/jrBoQdJklSfiYIkSRW1QKBgoiBJkuozUZAkqaK2Frg+0kRBkiTVZaIgSVJFvT9PMFGQJEkdMFGQ\nJKmiFpiiYKIgSZLqM1GQJKmiVriFs4mCJEmqy0RBkqSK2np/oGCiIEmS6jNRkCSpIucoSJKklmai\nIElSRd5HQZIktTQTBUmSKnKOgnq834w9lwP2+xgHjtqHb37tK7z++uvcOeEOPn7QARxy4CiO+NfD\neHLq1EXbX3vN1Ryw794csN/HOP7rX233mA8+cD//sv++7LPnbpz2/VPJMnub8+KLfP6zR7LvXrvz\n+c8eyUtz5gCQmZz2/VPZZ8/dOOiAfXnowQca/8Gld2m3D23KvZedyP3jTuJrR+72jvXL9e/Hb047\nkvvHncQt532NdddcbdG6r31md+4fdxL3XnYiu+6waZePKTUjC4VebMaMGfzu/PO44MJLuHTcH2lr\nW8A1V1/Fqad8hx/88CdceOk49v7YPvzfL38BwNSpT3D2/41h7G8v4LIrruLrx/9Hu8c99ZTvcNLJ\n3+XKP13Hk1Of4C+33QLAOb8aw3Yf3IEr/3Qd231wB87+1RgAbrv1Fp6c+gRX/uk6vv2d73LqKd9Z\nJp9fqqpPn+CM4w9h1DFnsfW/nMrBe76fTTZY423bfHr/HZj98jw2H3UyPz//Jr7376MA2GSDNTh4\nj23Y5qDvsd/RZ/HTbx1Cnz7RpWOq52nLxi/dzUKhl1uwYAGvv/Ya8+fPZ95rrzF02DAi4JW5rwDw\nyiuvMHTYMAAuvehCDj3sk6w6cCAAQ4YMecfxnntuJnPnvsIWW25FRLDvfvtz4/jxANx003j2239/\nAPbbf39uuvGGov3G8ey73/5EBFtsuRUvv/wSzz03s+GfXarqA5uvx2NPPc8T02fx5vwFXHTt3eyz\n0xZv22afnbbg/CvvBODSGyax03YbL2q/6Nq7eePN+Ux9ehaPPfU8H9h8vS4dUz1PLoP/dbeGzVGI\niE2AUcCIsmk6cEVmPtSoc+rthg8fzhGf/gx77LozK6ywPDt8aEc+tOOH+c4p3+OYL4xm+RWWZ+UB\nK/ObCy4EikQB4IhPHsqCtja++G/HsONHPvq2Y86cMYPhw9/6K2j4Gmswc+YMAF6YNYuhQ4uiY/XV\nh/LCrFnFPjNnMHyNmn2Gr8HMGTMWbSs1m7WGDWTajNmL3k+fMZvtNl/vnds8W2yzYEEbL70yjyGD\nBjBi6EDuvO+Jt/adOZu1hhXFd2fHlJpRQxKFiPgm8HsggL+WSwAXRMTxjTin3umlOXO46cbxXH3d\neK6/6VbmzZvHH68cx2/OO5cz/3cM1994C6MOOJCf/OgHAMxfsICpT07lV+f+htN+/F+c/J0Teeml\nlyqdOyIgYml+HElqOpmNX7pbo4YejgI+kJmnZeZvy+U0YLtyXbsiYnRETIyIiWPGjGlQ11rHhAm3\nM2LttVlttdXo378/u+y6O/dMupu/P/IwW2yxJQB77Lk3906aBBQJxE47j6R///6svfY6vOc96/Fk\nmTIsNGz4cGbMeHbR+xnPPsuwYcMBWG3IkEVDCs89N5PVVismdw0bNpwZz9bsM+NZhg0f3rDPLb1b\nT8+cw9rDBy96P2L4YKY/N+ed26xRbNO3bx9WXXlFZr04l+nPvdUOMGLYYJ6eOadLx5SaUaMKhTZg\nrXba1yzXtSszx2Tmtpm57ejRoxvUtdaxxpprMfnee5k3bx6ZyZ0T7mCD927IKy+/zBNP/AOAO+74\nC+tv8F4ARo7clYl//SsAs2e/wNSpT7D2Ouu87ZhDhw5jwICVmXzvPWQmV15xOTuP3AWAnXYeyRWX\nXw7AFZdfzs47v9V+5RWXk5lMvvceVl55FYcd1NQmPjCVDdcdynvWGkL/fn05eI9tuOrmyW/b5qo/\n38cn9/0gAAfuujV/vuvvRfvNkzl4j21Yrn8/3rPWEDZcdyh33f9El46pnqcVEoVGzVE4FhgfEY8C\nT5Vt6wIbAsc06JxazBZbbMluu+/BoQcfQN++/dhk00056OCPM3z4Gnz12C/TJ4JVBw7k5O9+H4AP\nffgj3H77Xzhg373p07cvx331GwwaVPwFdMiBo7jw0nEAnHDiSZx4wrd4/fXX2PHDH+XD5TyGz3x2\nNF//yrFcfunFrLnWWvz4v84A4CMf/Wduu+XP7LPXbqywwoqccur3u+G3IXXdggVtHPfDC7nyrKPp\n2ycYO24CDz3+LCd+8WPc/eCTXPXn+zj38ts559RPcf+4k5j90lwOP/7XADz0+LNcct0kJl1yAvMX\ntHHsaRfS1pZAtntMqdlFNqhciYg+FEMNtZMZ78rMBV08RL42vyFdk3q8FcoSf8Wtrbul9sybdCYU\nc+Ma6uoHZjb8b/693zesWyd8Neyqh8xsAyY06viSJKnxvIWzJEkVNcMcgkbzhkuSJKkuEwVJkipq\nhjsnNpqJgiRJqstEQZKkipyjIEmSmlpEDIqIiyPi4Yh4KCJ2iIjVIuL6iHi0/Dm43DYi4mcRMSUi\nJkfENp0d30JBkqSK2siGL13wU+CazNwE2BJ4CDgeGJ+ZGwHjy/cAewEblcto4BedHdxCQZKkHioi\nBgIfBc4GyMw3MvNFiqc3jy03GwvsX74eBZyXhQnAoIhYs6NzWChIklTRsnjWQ+0DE8ul9mFI6wPP\nAb+OiEkR8auIGAAMz8xnym2eBRY+iW8Ebz1aAWAab91BuV1OZpQkqYll5hig3iOV+wHbAF/KzDsj\n4qe8NcywcP+MiMrTLk0UJEmqKJfB0olpwLTMvLN8fzFF4TBj4ZBC+XNmuX46UPtY4LXLtrosFCRJ\n6qEy81ngqYjYuGzaBXgQuAI4omw7AhhXvr4C+FR59cP2wJyaIYp2OfQgSVJFjXoC8xL6EnB+RCwH\nPA4cSREEXBgRRwFTgUPKba8G9gamAK+W23bIQkGSpB4sM+8Btm1n1S7tbJvA0UtyfAsFSZIqauvu\nDiwDFgqSJFXUJEMPDeVkRkmSVJeJgiRJFfX+PMFEQZIkdcBEQZKkipyjIEmSWpqJgiRJFbXC5ZEm\nCpIkqS4TBUmSKnKOgiRJamkmCpIkVdQCgYKJgiRJqs9EQZKkilogUDBRkCRJ9ZkoSJJUUVsLTFIw\nUZAkSXWZKEiSVFHvzxNMFCRJUgdMFCRJqsg7M0qSpJZmoiBJUkWt8PRICwVJkipqgZEHhx4kSVJ9\nJgqSJFXkDZckSVJLM1GQJKmiFggUTBQkSVJ9JgqSJFXkHAVJktTSTBQkSaqorfcHCiYKkiSpPhMF\nSZIqaoEpCiYKkiSpPhMFSZIqaqP3RwomCpIkqS4TBUmSKnKOgiRJamkmCpIkVeR9FCRJUkszUZAk\nqSKf9SBJklqaiYIkSRW1QKBgoSBJUlVOZpQkSS3NREGSpIqyBcYeTBQkSerBIuKJiLgvIu6JiIll\n22oRcX1EPFr+HFy2R0T8LCKmRMTkiNims+NbKEiSVFFbNn7pop0zc6vM3LZ8fzwwPjM3AsaX7wH2\nAjYql9HALzo7sIWCJEm9zyhgbPl6LLB/Tft5WZgADIqINTs6kIWCJEkVNUmikMB1EfG3iBhdtg3P\nzGfK188Cw8vXI4CnavadVrbV5WRGSZKaWPnlP7qmaUxmjql5/+HMnB4Rw4DrI+Lh2v0zMyOi8qxL\nCwVJkipKGn/VQ1kUjOlg/fTy58yIuAzYDpgREWtm5jPl0MLMcvPpwDo1u69dttXl0IMkST1URAyI\niFUWvgZ2B+4HrgCOKDc7AhhXvr4C+FR59cP2wJyaIYp2mShIklRRE9yZcThwWURA8Z3+u8y8JiLu\nAi6MiKOAqcAh5fZXA3sDU4BXgSM7O0HdQiEiXoZFmUqUP7N8nZm56hJ/HEmStNRk5uPAlu20zwJ2\naac9gaOX5Bx1C4XMXGVJDiRJUqtpgRszdm2OQkR8OCKOLF+vHhHrN7ZbkiSpGXQ6RyEiTgK2BTYG\nfg0sB/wW2LGxXZMkqbm1tUCk0JVE4QBgP2AuQGY+DTgsIUlSC+jKVQ9v1N6sobz8QpKkltcEVz00\nXFcShQsj4pcU94P+HHAD8H+N7ZYkSWoGnSYKmfmTiNgNeAn4J+DbmXl9w3smSVKTa4EpCl2+4dJ9\nwIoU91G4r3HdkSRJzaTToYeI+CzwV+BA4CBgQkR8ptEdkySp2bVlNnzpbl1JFL4ObF3e5YmIGALc\nDpzTyI5JktTsmuB7vOG6MplxFvByzfuXyzZJktTLdfSsh6+UL6cAd0bEOIo5CqOAycugb5IkNbW2\n7u7AMtDR0MPCmyo9Vi4LjWtnW0mS1At19FCok5dlRyRJ6mmaYbJho3XlWQ9DgW8A7wNWWNiemSMb\n2C9JktQEujKZ8XzgYWB94GTgCeCuBvZJkqQeIbPxS3frSqEwJDPPBt7MzD9n5mcA0wRJklpAV+6j\n8Gb585mI+BjwNLBa47okSVLP0AoPhepKoXBqRAwEvgr8HFgVOK6hvZIkSU2hKw+F+mP5cg6wc2O7\nI0lSz5HNMImgwTq64dLPKW6w1K7M/HJDeiRJkppGR4nCxGXWC0mSeqCWnqOQmWOXZUckSVLz6cpk\nRkmS1I5WSBS6ch8FSZLUopo6UVihqXsndb95k87s7i5ILc2rHrzqQZKkltbUVz2suPUx3d0FqSkt\nTBJufHhWN/dEak4jNxmyTM7TtkzO0r286kGSJNXV1cdMfxPYDB8zLUnSIq0wR6Grj5l+CB8zLUlS\ny/Ex05IkVZTZ+KW7+ZhpSZIqamuGb/IG8zHTkiSpLh8zLUlSRS0QKHTpqodf086Nl8q5CpIkqRfr\nytDDH2terwAcQDFPQZKkltYKl0d2Zejhktr3EXEBcFvDeiRJkppGlccubQQMW9odkSSpp2mBQKFL\ncxRe5u1zFJ6luFOjJEnq5boy9LDKsuiIJEk9TSvcR6HTOzNGxPiutEmSpN6nbqIQESsAKwGrR8Rg\nIMpVqwIjlkHfJElqar0/T+h46OHzwLHAWsDfeKtQeAk4s8H9kiRJTaBuoZCZPwV+GhFfysyfL8M+\nSZLUI7TCfRS68vTItogYtPBNRAyOiH9rYJ8kSVKT6Eqh8LnMfHHhm8ycDXyucV2SJKlnaMvGL92t\nK4VC34hYOD+BiOgLLNe4LkmSpCUREX0jYlJE/LF8v35E3BkRUyLiDxGxXNm+fPl+Srl+vc6O3ZVC\n4RrgDxGxS0TsAlxQtkmS1NIys+FLF/078FDN+x8Cp2fmhsBs4Kiy/Shgdtl+erldh7pSKHwTuBH4\nYrmMB77e1Z5LkqTGiYi1gY8BvyrfBzASuLjcZCywf/l6VPmecv0utaMG7em0UMjMtsz838w8KDMP\nAh4EvApCktTyMhu/RMToiJhYs4xerBtnAN8A2sr3Q4AXM3N++X4ab93/aATwVNH3nA/MKbevq0sP\nhYqIrYHDgEOAfwCXdmU/SZL07mTmGGBMe+siYh9gZmb+LSJ2asT5O7oz4z9RFAeHAc8DfwAiM3du\nREckSeppmuA+CjsC+0XE3sAKFHdP/ikwKCL6lanB2sD0cvvpwDrAtIjoBwwEZnV0go6GHh6mGOPY\nJzM/XN50acG7+TSSJPUm3X15ZGZ+KzPXzsz1gEOBGzPzk8BNwEHlZkcA48rXV5TvKdffmJ1UOx0V\nCgcCzwA3RcT/lVc8dDjhQZIkNYVvAl+JiCkUcxDOLtvPBoaU7V8Bju/sQB3dwvly4PKIGEAxS/JY\nYFhE/AK4LDOve3efQZKknq0Jhh4WycybgZvL148D27WzzWvAwUty3K5c9TA3M3+XmftSjHNMoqhU\nJElSL9eV+ygskpmzM3NMZu7SqA5JktRT5DJYutsSFQqSJKm1dOk+CpIk6Z3ammiOQqOYKEiSpLpM\nFCRJqqgFAgUTBUmSVJ+JgiRJFTXTfRQaxURBkiTVZaIgSVJFLRAomChIkqT6TBQkSarI+yhIkqSW\nZqIgSVJFLRAomChIkqT6TBQkSarI+yhIkqSWZqIgSVJFbb0/ULBQkCSpqqT3VwoOPUiSpLpMFCRJ\nqqgF5jKaKEiSpPpMFCRJqsjLIyVJUkszUZAkqaJWuDzSREGSJNVloiBJUkXOUZAkSS3NREGSpIpa\nIFAwUZAkSfWZKEiSVFFbC0QKJgqSJKkuEwVJkipqgUDBREGSJNVnoiBJUkXeR0GSJLU0EwVJkipq\ngUDBREGSJNVnoiBJUkWtMIOICdoAABK0SURBVEfBQkGSpIpaoE5w6EGSJNVnoiBJUkWtMPRgoiBJ\nkuoyUZAkqSITBUmS1NJMFCRJqqgFAgUTBUmSeqqIWCEi/hoR90bEAxFxctm+fkTcGRFTIuIPEbFc\n2b58+X5KuX69zs5hoSBJUkWZ2fClE68DIzNzS2ArYM+I2B74IXB6Zm4IzAaOKrc/Cphdtp9ebtch\nCwVJknqoLLxSvu1fLgmMBC4u28cC+5evR5XvKdfvEhHR0TksFCRJqiiz8UtEjI6IiTXL6No+RETf\niLgHmAlcDzwGvJiZ88tNpgEjytcjgKeKvud8YA4wpKPP6GRGSZKaWGaOAcZ0sH4BsFVEDAIuAzZZ\nmue3UJAkqaJmuo9CZr4YETcBOwCDIqJfmRqsDUwvN5sOrANMi4h+wEBgVkfHdehBkqQeKiKGlkkC\nEbEisBvwEHATcFC52RHAuPL1FeV7yvU3ZifVjomCJEkVNUGgsCYwNiL6Uvzxf2Fm/jEiHgR+HxGn\nApOAs8vtzwZ+ExFTgBeAQzs7gYWCJEk9VGZOBrZup/1xYLt22l8DDl6Sc1goSJJUUTPNUWgU5yhI\nkqS6TBQkSaqoBQIFEwVJklSfiYIkSRW1whwFCwVJkipqgTrBoQdJklSfiYIkSRW1wtCDiYIkSarL\nREGSpIpaIFAwUZAkSfWZKLSo3T60KT/5+kH07dOHcy+/nZ/8+vq3rV+ufz/O/u7hbL3purwwZy7/\n+s1zePKZFwD42md259OjdmBBWxtf/dHF3HDHQ106ptSMXn3lZX575g94+snHiQgO/9J/cP/f7mDy\nnbcSffqwysBBfOrL/8mgIUO57tLzueuW6wBYsGA+z06byo/Pu5oBq6z6tmM+P+Npzv7xt5n78hzW\nfe8mfPq4b9Ovf3/efPMNxp7+XZ587GEGrDKQz379uwwZviYA11x8HrdffyXRpy8f/9yxbLbN9sv8\nd6El5xwF9Up9+gRnHH8Io445i63/5VQO3vP9bLLBGm/b5tP778Dsl+ex+aiT+fn5N/G9fx8FwCYb\nrMHBe2zDNgd9j/2OPouffusQ+vSJLh1TakYX/uoMNttme75z1u854YzzWGPt9djtgE/ynz/7DSec\nMZbNt92Rq//wawB2P/CTnHDGWE44Yyz7H/5FNnrfVu8oEgAuG3sWI/f7OKf88iJWWnkV/nLDlQDc\nfv2VrLTyKpzyy4sYud/HuWzsWQA88+Q/mHjrDZx45vl86Tv/zQW//AltCxYsu1+C1AELhRb0gc3X\n47GnnueJ6bN4c/4CLrr2bvbZaYu3bbPPTltw/pV3AnDpDZPYabuNF7VfdO3dvPHmfKY+PYvHnnqe\nD2y+XpeOKTWbeXNfYcoD97DjbvsC0K9/f1ZaeRVWXGnAom3eeP01iHjHvnfdej0f+Ohu72jPTB6Z\n/De22XFnALYfuRf3TrgFgHvvvJXtR+4FwDY77szDkyeSmdz711vZ9iO70r//cqw+fC2GrrE2Tzz6\n4FL/vFr6Mhu/dDeHHlrQWsMGMm3G7EXvp8+YzXabr/fObZ4ttlmwoI2XXpnHkEEDGDF0IHfe98Rb\n+86czVrDBgJ0ekyp2Tw/42lWHjiI8372Pab941HWfe8mHPK5Y1l+hRUZ95v/5c6brmGFAQM47tQz\n37bfG6+/xoN3T+DQ0V99xzHnvjyHlQasTN++xX9eBw0ZxosvPAfAiy88x+DVhwPQt28/VhwwgLkv\nz+HFWc+x/sbvW3SMwasP48VZzzXqY0tLZJknChFx5LI+pyS1p23BAp567O98dM8DOOGMsSy/wgpc\ne8lvABh1+Bf4/jmXs90/78HNV13ytv0m//U23rvpFu0OO6i1ZGbDl+7WHUMPJ9dbERGjI2JiREwc\nM2bMsuxTS3l65hzWHj540fsRwwcz/bk579xmjWKbvn37sOrKKzLrxblMf+6tdoARwwbz9Mw5XTqm\n1GwGrT6MQasPXfTX/NYf2pmnHnvkbdts98+7M+mOm97WNvHWG9j2I+8cdgAYsMpAXp37CgsWzAfg\nxVkzGbTa0OJ8qw1l9vMzgGIy5Ly5cxmwykAGDRnK7OdnLjrG7OdnMmjI0KXzIaV3qSGFQkRMrrPc\nBwyvt19mjsnMbTNz29GjRzeiawImPjCVDdcdynvWGkL/fn05eI9tuOrmyW/b5qo/38cn9/0gAAfu\nujV/vuvvRfvNkzl4j21Yrn8/3rPWEDZcdyh33f9El44pNZuBg4cwePXhPDttKgCPTJ7IGuusz8yn\nn1q0zb133soaI96z6P28ua/w6AOT2PKDH2n3mBHBxv9vG+7+S1FcTLjxT4u23WK7jzDhxj8BcPdf\nbmLjLd5PRLDFdh9m4q038Oabb/D8jKeZ+cw01ttos4Z8Zi1drZAoNGqOwnBgD2D2Yu0B3N6gc6qL\nFixo47gfXsiVZx1N3z7B2HETeOjxZznxix/j7gef5Ko/38e5l9/OOad+ivvHncTsl+Zy+PHFrO+H\nHn+WS66bxKRLTmD+gjaOPe1C2toSyHaPKTW7j3/uOH793yezYP6brL7GWhz+5RP47ZmnMWP6VPpE\nH1Ybtgaf+OI3Fm1/z4Q/s+lW27H8Ciu+7ThnnvJV/vXo4xk0ZCj7H/FvnP2Tb3Pl+WNYZ4N/4kPl\nZMkdd9uHc08/hW9//mBWWmVVjvraKQCste4GvH/HkZxyzCfo06cfh37+q/Tp23fZ/RKkDkQjqpWI\nOBv4dWbe1s6632XmJ7pwmFxx62OWet+k3mDepGJy3Y0Pz+rmnkjNaeQmQ6D447ShNv/P6xv+J//9\np+7W8M/RkYYkCpl5VAfrulIkSJKkJuDlkZIkVdQMcwgazRsuSZKkukwUJEmqqAUCBRMFSZJUn4mC\nJEkVFZeH924mCpIkqS4TBUmSKmqFOQoWCpIkVeTlkZIkqaWZKEiSVFELBAomCpIkqT4TBUmSKnKO\ngiRJamkmCpIkVdQCgYKJgiRJqs9EQZKkipyjIEmSWpqJgiRJFZkoSJKklmaiIElSVb0/UDBRkCRJ\n9ZkoSJJUkXMUJElSSzNRkCSpIhMFSZLU0iwUJEmqKDMbvnQkItaJiJsi4sGIeCAi/r1sXy0iro+I\nR8ufg8v2iIifRcSUiJgcEdt09hktFCRJ6rnmA1/NzM2A7YGjI2Iz4HhgfGZuBIwv3wPsBWxULqOB\nX3R2AgsFSZIq6u5EITOfycy7y9cvAw8BI4BRwNhys7HA/uXrUcB5WZgADIqINTs6h4WCJElVZeOX\niBgdERNrltHtdSUi1gO2Bu4EhmfmM+WqZ4Hh5esRwFM1u00r2+ryqgdJkppYZo4BxnS0TUSsDFwC\nHJuZL0VE7f4ZEZUvz7BQkCSpoma4PDIi+lMUCedn5qVl84yIWDMznymHFmaW7dOBdWp2X7tsq8uh\nB0mSeqgoooOzgYcy879rVl0BHFG+PgIYV9P+qfLqh+2BOTVDFO0yUZAkqaImSBR2BA4H7ouIe8q2\n/wBOAy6MiKOAqcAh5bqrgb2BKcCrwJGdncBCQZKkHiozbwOizupd2tk+gaOX5BwWCpIkVdQEiULD\nOUdBkiTVZaIgSVJVvT9QMFGQJEn1mShIklSRcxQkSVJLM1GQJKkiEwVJktTSTBQkSarIREGSJLU0\nEwVJkioyUZAkSS3NREGSpKp6f6BgoiBJkuozUZAkqaJWmKNgoSBJUkWtUCg49CBJkuoyUZAkqSIT\nBUmS1NJMFCRJqqr3BwomCpIkqT4TBUmSKnKOgiRJamkmCpIkVWSiIEmSWpqJgiRJFZkoSJKklmai\nIElSRSYKkiSppZkoSJJUVe8PFEwUJElSfSYKkiRV5BwFSZLU0kwUJEmqyERBkiS1NBMFSZIqaoVE\nwUJBkqSKWqFQcOhBkiTVZaIgSVJVvT9QMFGQJEn1mShIklSRcxQkSVJLM1GQJKkiEwVJktTSTBQk\nSarKREGSJLUyCwVJkqrKtsYvnYiIcyJiZkTcX9O2WkRcHxGPlj8Hl+0RET+LiCkRMTkituns+BYK\nkiT1bOcCey7WdjwwPjM3AsaX7wH2AjYql9HALzo7uIWCJElVZTZ+6bQLeQvwwmLNo4Cx5euxwP41\n7edlYQIwKCLW7Oj4FgqSJPU+wzPzmfL1s8Dw8vUI4Kma7aaVbXV51YMkSVV1YQ7BuxURoymGCRYa\nk5ljurp/ZmZEVL48w0JBkqQmVhYFXS4MSjMiYs3MfKYcWphZtk8H1qnZbu2yrS6HHiRJqqoJ5ijU\ncQVwRPn6CGBcTfunyqsftgfm1AxRtMtEQZKkHiwiLgB2AlaPiGnAScBpwIURcRQwFTik3PxqYG9g\nCvAqcGRnx7dQkCSpqmUwR6HTLmQeVmfVLu1sm8DRS3J8hx4kSVJdJgqSJFXVBIlCo5koSJKkukwU\nJEmqqgWeHmmhIElSVS0w9NDUhcK8SWd2dxekpjZykyHd3QVJvVwzFwrR3R3Q20XE6CW5bajUavw3\n0oJaYOjByYxaEqM730Rqaf4bUa/TzImCJEnNrQXmKJgoSJKkukwUtCQce5U65r+RVuMcBektTtKS\nOua/EfVGJgqSJFXlHAVJktTKLBTUqYjYMyIeiYgpEXF8d/dHaiYRcU5EzIyI+7u7L+oGmY1fupmF\ngjoUEX2B/wH2AjYDDouIzbq3V1JTORfYs7s7ITWKcxTUme2AKZn5OEBE/B4YBTzYrb2SmkRm3hIR\n63V3P9RNnKMgMQJ4qub9tLJNktQCTBQkSaqqCeYQNJqJgjozHVin5v3aZZskqQWYKKgzdwEbRcT6\nFAXCocAnurdLktQknKOgVpeZ84FjgGuBh4ALM/OB7u2V1Dwi4gLgDmDjiJgWEUd1d5+kpclEQZ3K\nzKuBq7u7H1IzyszDursP6kZtzlGQJEktzERBkqSqWmCOgoWCJElVtUCh4NCDJEmqy0RBkqSqvOGS\npIUiYkFE3BMR90fERRGx0rs41rkRcVD5+lcdPWgrInaKiA9VOMcTEbF6V9sX2+aVJTzXdyLia0va\nR0nNz0JB6rp5mblVZm4OvAF8oXZlRFRK6DLzs5nZ0UO2dgKWuFCQtAxkW+OXbmahIFVzK7Bh+df+\nrRFxBfBgRPSNiB9HxF0RMTkiPg8QhTMj4pGIuAEYtvBAEXFzRGxbvt4zIu6OiHsjYnz5VMIvAMeV\nacZHImJoRFxSnuOuiNix3HdIRFwXEQ9ExK+A6OxDRMTlEfG3cp/Ri607vWwfHxFDy7b3RsQ15T63\nRsQmS+OXKal5OUdBWkJlcrAXcE3ZtA2weWb+o/yynZOZH4iI5YG/RMR1wNbAxsBmwHCKx3Sfs9hx\nhwL/B3y0PNZqmflCRPwv8Epm/qTc7nfA6Zl5W0SsS3HXzE2Bk4DbMvOUiPgY0JU7BH6mPMeKwF0R\ncUlmzgIGABMz87iI+HZ57GOAMcAXMvPRiPggcBYwssKvUeodWmCOgoWC1HUrRsQ95etbgbMphgT+\nmpn/KNt3B7ZYOP8AGAhsBHwUuCAzFwBPR8SN7Rx/e+CWhcfKzBfq9GNXYLOIRYHBqhGxcnmOA8t9\nr4qI2V34TF+OiAPK1+uUfZ0FtAF/KNt/C1xanuNDwEU1516+C+eQ1INZKEhdNy8zt6ptKL8w59Y2\nAV/KzGsX227vpdiPPsD2mflaO33psojYiaLo2CEzX42Im4EV6mye5XlfXPx3ILW0JphD0GjOUZCW\nrmuBL0ZEf4CI+KeIGADcAny8nMOwJrBzO/tOAD5aPqmTiFitbH8ZWKVmu+uALy18ExELv7hvoXyy\nZ0TsBQzupK8DgdllkbAJRaKxUB9gYSryCYohjZeAf0TEweU5IiK27OQckno4CwVp6foVxfyDuyPi\nfuCXFMndZcCj5brzKJ42+DaZ+RwwmiLmv5e3ov8rgQMWTmYEvgxsW06WfJC3rr44maLQeIBiCOLJ\nTvp6DdAvIh4CTqMoVBaaC2xXfoaRwCll+yeBo8r+PQCM6sLvROq9Mhu/dLPIJuiEJEk90Yrbf7Ph\nX6LzJvxwycYVlzLnKEiSVJVzFCRJUiszUZAkqaoWGL43UZAkSXWZKEiSVJVzFCRJUiszUZAkqSrn\nKEiSpFZmoiBJUlXOUZAkSXU1wS2cI2LPiHgkIqZExPFL+yNaKEiS1ENFRF/gf4C9gM2AwyJis6V5\nDoceJEmqqvuHHrYDpmTm4wAR8XuKh7U9uLROYKIgSVLPNQJ4qub9tLJtqTFRkCSponmTzmz4kx0j\nYjTFI+gXGpOZYxp93oUsFCRJamJlUVCvMJgOrFPzfu2ybalx6EGSpJ7rLmCjiFg/IpYDDgWuWJon\nMFGQJKmHysz5EXEMcC3QFzgnMx9YmueIbIHbT0qSpGocepAkSXVZKEiSpLosFCRJUl0WCpIkqS4L\nBUmSVJeFgiRJqstCQZIk1WWhIEmS6vr/QYNsmuDdZgMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 648x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}